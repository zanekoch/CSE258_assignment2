{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Data reading methods</h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def padding_sequence(seq, max_len = 501, repkey = 'N'):\n",
    "    seq_len = len(seq)\n",
    "    if seq_len < max_len:\n",
    "        gap_len = max_len -seq_len\n",
    "        new_seq = seq + repkey * gap_len\n",
    "    else:\n",
    "        new_seq = seq[:max_len]\n",
    "    return new_seq"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def get_RNA_seq_concolutional_array(seq, motif_len = 4):\n",
    "    seq = seq.replace('U', 'T')\n",
    "    alpha = 'ACGT'\n",
    "    #for seq in seqs:\n",
    "    #for key, seq in seqs.iteritems():\n",
    "    row = (len(seq) + 2*motif_len - 2)\n",
    "    new_array = np.zeros((row, 4))\n",
    "    for i in range(motif_len-1):\n",
    "        new_array[i] = np.array([0.25]*4)\n",
    "    \n",
    "    for i in range(row-3, row):\n",
    "        new_array[i] = np.array([0.25]*4)\n",
    "        \n",
    "    #pdb.set_trace()\n",
    "    for i, val in enumerate(seq):\n",
    "        i = i + motif_len-1\n",
    "        # add .25's because this val is an N\n",
    "        if val not in 'ACGT':\n",
    "            new_array[i] = np.array([0.25]*4)\n",
    "            continue\n",
    "        #if val == 'N' or i < motif_len or i > len(seq) - motif_len:\n",
    "        #    new_array[i] = np.array([0.25]*4)\n",
    "        #else:\n",
    "        try:\n",
    "            index = alpha.index(val)\n",
    "            new_array[i][index] = 1\n",
    "        except:\n",
    "            pdb.set_trace()\n",
    "        #data[key] = new_array\n",
    "    return new_array\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def get_bag_data_1_channel(data, max_len = 501):\n",
    "    bags = []\n",
    "    seqs = data[\"seq\"]\n",
    "    labels = data[\"Y\"]\n",
    "    for seq in seqs:\n",
    "        #pdb.set_trace()\n",
    "        #bag_seqs = split_overlap_seq(seq)\n",
    "        \n",
    "        # replace each sequence with the sequence followed by N's until the length is equal to max_len\n",
    "        bag_seq = padding_sequence(seq, max_len = max_len)\n",
    "        #flat_array = []\n",
    "        bag_subt = []\n",
    "        #for bag_seq in bag_seqs:\n",
    "        \n",
    "        # turn the padded sequence into a 2-D array, where a row of the array is [.25,.25,.25,.25] for an N, and [1,0,0,0] for an A, [0,1,0,0] for an T, and so on\n",
    "        tri_fea = get_RNA_seq_concolutional_array(bag_seq)\n",
    "        bag_subt.append(tri_fea.T)\n",
    "\n",
    "        \n",
    "        bags.append(np.array(bag_subt))\n",
    "    \n",
    "        \n",
    "    return bags, labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def read_seq_graphprot(seq_file, label = 1):\n",
    "    seq_list = []\n",
    "    labels = []\n",
    "    seq = ''\n",
    "    with open(seq_file, 'r') as fp:\n",
    "        for line in fp:\n",
    "            if line[0] == '>':\n",
    "                name = line[1:-1]\n",
    "            else:\n",
    "                seq = line[:-1].upper()\n",
    "                seq = seq.replace('T', 'U')\n",
    "                seq_list.append(seq)\n",
    "                labels.append(label)\n",
    "    \n",
    "    return seq_list, labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def read_data_file(posifile, negafile = None, train = True):\n",
    "    data = dict()\n",
    "    seqs, labels = read_seq_graphprot(posifile, label = 1)\n",
    "    if negafile:\n",
    "        seqs2, labels2 = read_seq_graphprot(negafile, label = 0)\n",
    "        seqs = seqs + seqs2\n",
    "        labels = labels + labels2\n",
    "    # made a dictionary, \"seq\" key has a list of sequences as values, \"Y\" key has a list of 0's and 1's as values inidicating if the corresponding sequence is bound or not\n",
    "    data[\"seq\"] = seqs \n",
    "    data[\"Y\"] = np.array(labels)\n",
    "    \n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def get_data(posi, nega = None, channel = 1,  window_size = 101, train = True):\n",
    "    data = read_data_file(posi, nega, train = train)\n",
    "    if channel == 1:\n",
    "        # pad the sequences and turn them into one-hot-encoded 2D arrays\n",
    "        train_bags, label = get_bag_data_1_channel(data, max_len = window_size)\n",
    "\n",
    "    else:\n",
    "        train_bags, label = get_bag_data(data, channel = channel, window_size = window_size)\n",
    "    \n",
    "    return train_bags, label"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Data exploration</h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# TASK 1: data exploration\n",
    "train_data_dict = read_data_file('./CLIPSEQ_AGO2.train.positives.fa', './CLIPSEQ_AGO2.train.negatives.fa', train=True)\n",
    "test_data_dict = read_data_file('./CLIPSEQ_AGO2.ls.positives.fa', './CLIPSEQ_AGO2.ls.negatives.fa', train=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(\"The number of training samples is {} and the number of testing samples is {}\".format(len(train_data_dict[\"seq\"]), len(test_data_dict[\"seq\"])))\n",
    "\n",
    "sum = 0\n",
    "max = 0\n",
    "min = 1000\n",
    "for seq in train_data_dict[\"seq\"]:\n",
    "    sum += len(seq)\n",
    "    if len(seq) > max: max = len(seq)\n",
    "    if len(seq) < min: min = len(seq)\n",
    "avg_seq_len = sum/len(train_data_dict[\"seq\"])\n",
    "\n",
    "print(\"The average sequence length is {} basepairs with max length {} and min length {}\".format(avg_seq_len, max, min))\n",
    "\n",
    "pos = train_data_dict[\"Y\"].sum()\n",
    "pos_test = test_data_dict[\"Y\"].sum()\n",
    "print(\"The number of positive training samples is {}, negative {}, positive testing {}, negative testing {}\".format(pos, len(train_data_dict[\"Y\"])-pos, pos_test, len(test_data_dict[\"Y\"])-pos_test ))                                                                                                         "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of training samples is 92346 and the number of testing samples is 1000\n",
      "The average sequence length is 335.3414333051783 basepairs with max length 375 and min length 137\n",
      "The number of positive training samples is 48095, negative 44251, positive testing 500, negative testing 500\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Read in data and format for NN</h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# read in the data \n",
    "f, l = get_data('./CLIPSEQ_AGO2.train.positives.fa', './CLIPSEQ_AGO2.train.negatives.fa', channel=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "f_test, l_test = get_data('./CLIPSEQ_AGO2.ls.positives.fa','CLIPSEQ_AGO2.ls.negatives.fa', channel=1, train=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "f = np.array(f)\n",
    "f = np.swapaxes(f, -1, 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "f_test = np.array(f_test)\n",
    "f_test = np.swapaxes(f_test, -1, 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Model training</h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "import keras\n",
    "from keras import applications\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Provided model from the github</h3>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(128, kernel_size=(10, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=f.shape[1:], padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling2D(pool_size=(3, 1)))\n",
    "model.add(Conv2D(128, (10, 1), activation='relu', padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling2D(pool_size=(3, 1)))\n",
    "model.add(Conv2D(256, (5, 1), activation='relu', padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(keras.layers.GlobalAveragePooling2D())\n",
    "model.add(Dropout(0.25))\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 107, 4, 128)       3968      \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 107, 4, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 35, 4, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 35, 4, 128)        163968    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 35, 4, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 11, 4, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 11, 4, 256)        164096    \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 11, 4, 256)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_4 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 365,057\n",
      "Trainable params: 365,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "opt = SGD(lr=0.01)\n",
    "# model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=opt,metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "model.fit(f, l,\n",
    "          batch_size=10,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_data=(f_test, l_test))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "9235/9235 [==============================] - 431s 47ms/step - loss: 0.6875 - accuracy: 0.5435 - val_loss: 0.6786 - val_accuracy: 0.5960\n",
      "Epoch 2/5\n",
      " 166/9235 [..............................] - ETA: 7:03 - loss: 0.6807 - accuracy: 0.5747"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3468436/3343755572.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(f, l,\n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           validation_data=(f_test, l_test))\n",
      "\u001b[0;32m~/miniconda3/envs/deepLearning/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepLearning/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepLearning/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepLearning/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepLearning/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniconda3/envs/deepLearning/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepLearning/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Simple CNN Model</h3>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "simple_model = Sequential()\n",
    "# learning 128 different convolution filters => results 128 tensors output after this layer\n",
    "# s\n",
    "simple_model.add(Conv2D(128, kernel_size=(10, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=f.shape[1:], padding='same'))\n",
    "simple_model.add(Dropout(0.25))\n",
    "simple_model.add(keras.layers.GlobalAveragePooling2D())\n",
    "simple_model.add(Dense(128, activation='relu'))\n",
    "simple_model.add(Dropout(0.5))\n",
    "simple_model.add(Dense(1, activation='sigmoid'))\n",
    "simple_model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 107, 4, 128)       3968      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 107, 4, 128)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 20,609\n",
      "Trainable params: 20,609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "opt = SGD(lr=0.01)\n",
    "# model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])\n",
    "simple_model.compile(loss=keras.losses.binary_crossentropy, optimizer=opt,metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "simple_model.fit(f, l,\n",
    "          batch_size=10,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_data=(f_test, l_test))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "9235/9235 [==============================] - 100s 11ms/step - loss: 0.6880 - accuracy: 0.5427 - val_loss: 0.6848 - val_accuracy: 0.5600\n",
      "Epoch 2/5\n",
      "9235/9235 [==============================] - 100s 11ms/step - loss: 0.6846 - accuracy: 0.5589 - val_loss: 0.6785 - val_accuracy: 0.6000\n",
      "Epoch 3/5\n",
      "9235/9235 [==============================] - 100s 11ms/step - loss: 0.6822 - accuracy: 0.5660 - val_loss: 0.6813 - val_accuracy: 0.5670\n",
      "Epoch 4/5\n",
      "9235/9235 [==============================] - 100s 11ms/step - loss: 0.6810 - accuracy: 0.5706 - val_loss: 0.6825 - val_accuracy: 0.5580\n",
      "Epoch 5/5\n",
      "9235/9235 [==============================] - 100s 11ms/step - loss: 0.6803 - accuracy: 0.5720 - val_loss: 0.6784 - val_accuracy: 0.5820\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x155416d635e0>"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> ResNet Model </h3>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Baselines</h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "# hamming and edit distance baseline\n",
    "from tinyalign import edit_distance, hamming_distance"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "# edit distance baseline\n",
    "# done on raw, unpadded reads => length differences count toward edit distance\n",
    "correct_predictions = 0\n",
    "n = 1\n",
    "for seq, label in zip (test_data_dict[\"seq\"], test_data_dict[\"Y\"]):\n",
    "    min_dist = 1000\n",
    "    # calculate edit distance between test seq and each of the training sequences (only use 10,000 for computation time)\n",
    "    for train_seq, train_label in zip( train_data_dict[\"seq\"][:10000],  train_data_dict[\"Y\"][:10000] ):\n",
    "        dist = edit_distance(seq, train_seq)\n",
    "        if dist < min_dist: \n",
    "            min_dist = dist\n",
    "            min_dist_label = train_label\n",
    "    # will add one for each correct prediciton\n",
    "    if min_dist_label == label:\n",
    "        correct_predictions += 1\n",
    "    if n%100 == 0:\n",
    "        print(\"Percent done: {}%\".format(n/1000))\n",
    "    n+=1\n",
    "\n",
    "print(\"edit distance baseline validation accuracy {}\".format(correct_predictions/len(test_data_dict[\"Y\"])))\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Percent done: 0.1%\n",
      "Percent done: 0.2%\n",
      "Percent done: 0.3%\n",
      "Percent done: 0.4%\n",
      "Percent done: 0.5%\n",
      "Percent done: 0.6%\n",
      "Percent done: 0.7%\n",
      "Percent done: 0.8%\n",
      "Percent done: 0.9%\n",
      "Percent done: 1.0%\n",
      "edit distance baseline validation accuracy 0.5\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "# pad all the sequences\n",
    "padded_test_data_dict = {}\n",
    "padded_test_data_dict[\"Y\"] = test_data_dict[\"Y\"]\n",
    "padded_train_data_dict = {}\n",
    "padded_train_data_dict[\"Y\"] = train_data_dict[\"Y\"]\n",
    "\n",
    "padded_test = []\n",
    "for seq in test_data_dict[\"seq\"]:\n",
    "    padded_test.append(padding_sequence(seq))\n",
    "padded_test_data_dict[\"seq\"] = padded_test\n",
    "\n",
    "padded_train = []\n",
    "for seq in train_data_dict[\"seq\"]:\n",
    "    padded_train.append(padding_sequence(seq))\n",
    "padded_train_data_dict[\"seq\"] = padded_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "# hamming distance baseline\n",
    "# this is done on the reads once they have already been padded with N's, so they are all the same length\n",
    "correct_predictions = 0\n",
    "n = 1\n",
    "for seq, label in zip (padded_test_data_dict[\"seq\"], padded_test_data_dict[\"Y\"]):\n",
    "    min_dist = 1000\n",
    "    # calculate edit distance between test seq and each of the training sequences \n",
    "    for train_seq, train_label in zip(padded_train_data_dict[\"seq\"], padded_train_data_dict[\"Y\"]):\n",
    "        dist = hamming_distance(seq, train_seq)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            min_dist_label = train_label\n",
    "    # will add one for each correct prediciton\n",
    "    if min_dist_label == label:\n",
    "        correct_predictions += 1\n",
    "    if n%100 == 0:\n",
    "        print(\"Percent done: {}%\".format(n/1000))\n",
    "    n+=1\n",
    "\n",
    "print(\"hamming distance baseline validation accuracy {}\".format(correct_predictions/len(test_data_dict[\"Y\"])))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Percent done: 0.1%\n",
      "Percent done: 0.2%\n",
      "Percent done: 0.3%\n",
      "Percent done: 0.4%\n",
      "Percent done: 0.5%\n",
      "Percent done: 0.6%\n",
      "Percent done: 0.7%\n",
      "Percent done: 0.8%\n",
      "Percent done: 0.9%\n",
      "Percent done: 1.0%\n",
      "hamming distance baseline validation accuracy 0.635\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a>edit distance baseline validation accuracy 0.5</a>\n",
    "<a>hamming distance baseline validation accuracy 0.635</a>"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "703f02bdb79bc8f251e1db33fb6f1fa012df65abb324d1bf7de099e1be8da837"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('deepLearning': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}